{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with large data volumes\n",
    "\n",
    "> Authors: Ashley Smith\n",
    ">\n",
    "> Abstract: Some strategies for requesting and handling larger data volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code could take a long time to run, so it is better to adjust it for smaller jobs if you are just testing it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -i -v -p viresclient,pandas,xarray,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from viresclient import SwarmRequest\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the request parameters - magnetic data and model evaluations\n",
    "\n",
    "We fetch the measurements (`F`, `B_NEC`), and model values (named as `F_CHAOS`, `B_NEC_CHAOS`) - here we name the custom model as \"CHAOS\" but you can call it anything.\n",
    "\n",
    "It is also possible to fetch data from all satellites at once with `request.set_collection(\"SW_OPER_MAGA_LR_1B\", \"SW_OPER_MAGB_LR_1B\", \"\"SW_OPER_MAGC_LR_1B\")`, which will be identified in the returned data by the `Spacecraft` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = SwarmRequest()\n",
    "request.set_collection(\"SW_OPER_MAGA_LR_1B\")  # Swarm Alpha\n",
    "request.set_products(\n",
    "    measurements=[\"F\",],\n",
    "    # Choose between the full CHAOS model (will be a lot slower - the MMA part could use some optimisation(?))\n",
    "#     models=[\"CHAOS = 'CHAOS-6-Core' + 'CHAOS-6-Static' + 'CHAOS-6-MMA-Primary' + 'CHAOS-6-MMA-Secondary'\"],\n",
    "    # ...or just the core part:\n",
    "  #  models=[\"CHAOS = 'CHAOS-Core'\"],\n",
    "    sampling_step=\"PT60S\"\n",
    ")\n",
    "# Quality Flags\n",
    "# https://earth.esa.int/web/guest/missions/esa-eo-missions/swarm/data-handbook/level-1b-product-definitions#label-Flags_F-and-Flags_B-Values-of-MDR_MAG_LR\n",
    "# NB: will need to do something different for Charlie because the ASM broke so Flags_F are bad\n",
    "request.set_range_filter(\"Flags_F\", 0, 1)\n",
    "request.set_range_filter(\"Flags_B\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at one day to see what the output data will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = request.get_between(\n",
    "    start_time=dt.datetime(2014,1,1),\n",
    "    end_time=dt.datetime(2020,1,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.as_dataframe(expand=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.as_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three options suggested for how to deal with larger volumes (option 2 is recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. a) Fetch two years of data and save them directly\n",
    "\n",
    "For this example we simplify the acquired data (lower cadence, fewer measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = SwarmRequest()\n",
    "request.set_collection(\"SW_OPER_MAGA_LR_1B\")  # Swarm Alpha\n",
    "request.set_products(\n",
    "    measurements=[\"F\"],\n",
    "    sampling_step=\"PT5S\"\n",
    ")\n",
    "# Quality Flags\n",
    "# https://earth.esa.int/web/guest/missions/esa-eo-missions/swarm/data-handbook/level-1b-product-definitions#label-Flags_F-and-Flags_B-Values-of-MDR_MAG_LR\n",
    "# NB: will need to do something different for Charlie because the ASM broke so Flags_F are bad\n",
    "request.set_range_filter(\"Flags_F\", 0, 1)\n",
    "request.set_range_filter(\"Flags_B\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request is automatically split up and sequentially processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = request.get_between(\n",
    "    start_time=dt.datetime(2014,1,1),\n",
    "    end_time=dt.datetime(2016,1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` is a kind of wrapper around some temporary CDF files (more data -> more files). This means the data is accessible but not yet loaded into memory. When the variable `data` is deleted, or the current program is closed, the files will be removed. `data.contents` is a list of objects which point to each file, and each on-disk filename can be retrieved as below.\n",
    "\n",
    "Warning: this behaviour is likely to change in the future (an underscore in the name, `_file`, indicates a private variable whose behaviour in the future is not guaranteed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.contents[0]._file.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data.as_dataframe()` / `data.as_xarray()` will read the files and concatenate them, but will fail if you don't have the memory to load them all simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the files directly\n",
    "\n",
    "The length of `data.contents` tells us the number of temporary files. Use this to make up some file names to give them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f\"testfile_{n:03}.cdf\" for n in range(len(data.contents))]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.to_files.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_files(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here you may use some other tool to work with the files. I will just remove them now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm testfile_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. b) Tune the size of each generated file by making multiple requests manually\n",
    "\n",
    "Generate lists of start and end times to use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_start_ends(\n",
    "        start=dt.datetime(2014, 1, 1),\n",
    "        end=dt.datetime(2014, 2, 1),\n",
    "        ndays=1):\n",
    "    delta_t = dt.timedelta(days=ndays)\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    start_i = start\n",
    "    end_i = start_i + dt.timedelta(days=1)\n",
    "    while end_i <= end:\n",
    "    #     print(start, end)\n",
    "        start_times.append(start_i)\n",
    "        end_times.append(end_i)\n",
    "        start_i, end_i = end_i, end_i + delta_t\n",
    "    # Append an uneven ending segment if necessary\n",
    "    if end_times[-1] < end:\n",
    "        start_times.append(start_i)\n",
    "        end_times.append(end)\n",
    "    return start_times, end_times\n",
    "    \n",
    "start_times, end_times = gen_start_ends(\n",
    "    start=dt.datetime(2014, 1, 1),\n",
    "    end=dt.datetime(2014, 2, 1),\n",
    "    ndays=1\n",
    ")\n",
    "list(zip(start_times, end_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some file names to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f\"data_{start.strftime('%Y-%m-%d')}.cdf\" for start in start_times]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through these dates and file names to fetch and save each.\n",
    "\n",
    "(Here we remove the progress bars with `show_progress=False` just to keep this notebook cleaner - in reality you might want to keep them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start, end, filename in zip(start_times, end_times, filenames):\n",
    "    data = request.get_between(start, end, show_progress=False)\n",
    "    data.to_file(filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data_2014*.cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use viresclient to translate the data from CDF to xarray then to netCDF - using the chunks defined as above\n",
    "\n",
    "There are some nicer tools for working with netCDF4/HDF files in Python so this may be preferable. This is also a point at which you may do some pre-processing before saving the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times, end_times = gen_start_ends(\n",
    "    start=dt.datetime(2014, 1, 1),\n",
    "    end=dt.datetime(2014, 2, 1))\n",
    "filenames_nc = [f\"data_{start.strftime('%Y-%m-%d')}.nc\" for start in start_times]\n",
    "\n",
    "for start, end, filename in zip(start_times, end_times, filenames_nc):\n",
    "    try:\n",
    "        data = request.get_between(start, end, show_progress=False)\n",
    "        ds = data.as_xarray()\n",
    "        print(f\"saved {filename}\")\n",
    "    except RuntimeError:\n",
    "        print(f\"No data for {filename} - data not downloaded\")\n",
    "    try:\n",
    "        ds.to_netcdf(filename)\n",
    "    except AttributeError:\n",
    "        print(f\"No data for {filename} - file not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use xarray+dask to lazily load the data\n",
    "\n",
    "See https://xarray.pydata.org/en/stable/dask.html\n",
    "\n",
    "Note: there is currently a bug in loading data where the variables in the file are empty (Swarm Alpha had a few problem days in January 2014 - this does not happen often). We can identify these problem files like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob(\"data_*.nc\")\n",
    "empty_files = []\n",
    "for filename in filenames:\n",
    "    try:\n",
    "        xr.open_dataset(filename)\n",
    "    except ValueError:\n",
    "        empty_files.append(filename)\n",
    "empty_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the problem files above\n",
    "filenames = [f for f in filenames if f not in empty_files]\n",
    "filenames.sort()\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = xr.open_mfdataset(\"data_2014*.nc\", combine=\"by_coords\")\n",
    "ds = xr.open_mfdataset(filenames, combine=\"by_coords\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Loading in this way has lost the source information - only the first one has been kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have access to the dataset which is stored on disk as multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"F\"].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leave the handling to viresclient and just try to load the full data directly\n",
    "\n",
    "There are some performance issues here, and if the total size is too big for your machine's memory then it won't be possible. We could make some changes to viresclient in the future to perform the lazy loading as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = request.get_between(\n",
    "    start_time=dt.datetime(2014,1,1),\n",
    "    end_time=dt.datetime(2015,1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = data.as_xarray()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ds.Sources[:3]:\n",
    "    print(i)\n",
    "print(\"...\")\n",
    "for i in ds.Sources[-3:]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is possible to access each file and load as xarray Dataset (rather than automatically concatenating them all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.contents[0].as_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# filenames = [f\"testfile_{n:03}.nc\" for n in range(len(data.contents))]\n",
    "# for data_part, filename in zip(data.contents, filenames):\n",
    "#     data_part.as_xarray().to_netcdf(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = open_mfdataset(\"testfile*.nc\", combine=\"by_coords\")\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm testfile*.nc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
